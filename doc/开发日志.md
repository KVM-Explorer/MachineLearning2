## 算法测试

### 普通KNN算法
在k=3,采用

```
全部样本准确率为19.444444444444446%

类别0的精确率为13.636363636363635%
类别1的精确率为0%

类别2的精确率为28.57142857142857%

类别0的召回率为27.27272727272727%
类别1的召回率为0.0%
类别2的召回率为36.36363636363637%
```



|            | 识别为第一类样本 | 识别为第二类样本 | 识别为第三类样本 |
| ---------- | ---------------- | ---------------- | ---------------- |
| 第一类样本 | 3                | 0                | 8                |
| 第二类样本 | 12               | 0                | 2                |
| 第三类样本 | 7                | 0                | 4                |



### 线性变换映射算法

```
===============================

全部样本准确率为30.555555555555557%

类别0的精确率为30.555555555555557%
类别1的精确率为0%

类别2的精确率为0%

类别0的召回率为100.0%
类别1的召回率为0.0%
类别2的召回率为0.0%
```



|            | 识别为第一类样本 | 识别为第二类样本 | 识别为第三类样本 |
| ---------- | ---------------- | ---------------- | ---------------- |
| 第一类样本 | 11               | 0                | 0                |
| 第二类样本 | 14               | 0                | 0                |
| 第三类样本 | 11               | 0                | 0                |



### 总结1

线性变换使得算法识别能力变差，大幅度提高了第一类样本的的识别范围导致，二三类样本被误识别为第一类样本。总体正确率却随之从19%提升到30%，现阶段的问题我认为主要处在图像特征的提取方面，后续尝试改进。

找到问题所在，特征距离函数错误的采取了加法运算。

### 普通KNN算法

```
===============================

全部样本准确率为63.888888888888886%

类别0的精确率为66.66666666666666%
类别1的精确率为72.72727272727273%

类别2的精确率为50.0%

类别0的召回率为90.9090909090909%
类别1的召回率为57.14285714285714%
类别2的召回率为45.45454545454545%
```



|            | 识别为第一类样本 | 识别为第二类样本 | 识别为第三类样本 |
| ---------- | ---------------- | ---------------- | ---------------- |
| 第一类样本 | 10               | 1                | 0                |
| 第二类样本 | 1                | 8                | 5                |
| 第三类样本 | 4                | 2                | 5                |

### 线性变换映射算法

```
===============================

全部样本准确率为72.22222222222221%

类别0的精确率为73.33333333333333%
类别1的精确率为81.81818181818183%

类别2的精确率为60.0%

类别0的召回率为100.0%
类别1的召回率为64.28571428571429%
类别2的召回率为54.54545454545454%
```



|            | 识别为第一类样本 | 识别为第二类样本 | 识别为第三类样本 |
| ---------- | ---------------- | ---------------- | ---------------- |
| 第一类样本 | 11               | 0                | 0                |
| 第二类样本 | 1                | 9                | 4                |
| 第三类样本 | 3                | 2                | 6                |

### 总结2

识别正确率大幅度提升基本达到预期的基本水平，其后的线性变换依然保持其性能稳定的基础上提升10个百分点。

## 算法性能调优

### 算法性能调优

- ~~K值的选取——交叉验证实验~~
- 度量方式的选择
  - 选择
  - 线性变换适配
- 同类样本平均密度  ——特征显著程度
- 度量函数选择
- one class识别
- 异常样本剔除

### K值交叉验证

#### 目的

  选出最为适合的模型超参数的取值，然后将超参数的值作用到模型的创建中。

####  思想

  将样本的**训练数据**交叉的拆分出不同的训练集和验证集，使用交叉拆分出不同的训练集和验证集测分别试模型的精准度，然后求出的精准度的均值就是此次交叉验证的结果。将交叉验证作用到不同的超参数中，选取出精准度最高的超参数作为模型创建的超参数即可。

#### 实现思路

1. 将训练数据平均分割成K个等份
2. 使用1份数据作为验证数据，其余作为训练数据
3. 计算验证准确率
4. 使用不同的测试集，重复2、3步骤，直到所有等份都作为过训练数据
5. 对准确率做平均，作为对未知数据预测准确率的估计

#### 实验验证

当对训练集按block_size 进行分块,分成block_num训练块，



| block_size | block_num | best_score | best_k               | img                                                          |
| ---------- | --------- | ---------- | -------------------- | ------------------------------------------------------------ |
| 3          | 27        | 82.14      | 3,4,5,6,10           | <img src="https://markdown-image-1302476306.cos.ap-nanjing.myqcloud.com/202205041603300.png" alt="image-20220504160019936" style="zoom:50%;" /> |
| 4          | 20        | 76.19      | 4,6,7,8,9,10         | <img src="https://markdown-image-1302476306.cos.ap-nanjing.myqcloud.com/202205041604222.png" alt="image-20220504160426170" style="zoom:50%;" /> |
| 5          | 16        | 94.12      | 6,7.8,9,10           | <img src="https://markdown-image-1302476306.cos.ap-nanjing.myqcloud.com/202205041605680.png" alt="image-20220504160503608" style="zoom:50%;" /> |
| 6          | 13        | 92.68      | 3,4,5,6              | <img src="https://markdown-image-1302476306.cos.ap-nanjing.myqcloud.com/202205041606249.png" alt="image-20220504160634208" style="zoom:50%;" /> |
| 7          | 11        | 91.67      | 5,6,7,8              | <img src="https://markdown-image-1302476306.cos.ap-nanjing.myqcloud.com/202205041607849.png" alt="image-20220504160710810" style="zoom:50%;" /> |
| 8          | 10        | 81.82      | 4,5,6,7,8,9,10       | <img src="https://markdown-image-1302476306.cos.ap-nanjing.myqcloud.com/202205041611670.png" alt="image-20220504161121616" style="zoom:50%;" /> |
| 9          | 9         | 80.00      | 1, 2, 3, 4, 8, 9, 10 | <img src="https://markdown-image-1302476306.cos.ap-nanjing.myqcloud.com/202205041613620.png" alt="image-20220504161309583" style="zoom:50%;" /> |
| 10         | 8         | 88.89      | 6, 7, 8, 9, 10       | <img src="https://markdown-image-1302476306.cos.ap-nanjing.myqcloud.com/202205041612300.png" alt="image-20220504161243253" style="zoom:50%;" /> |

从准确率和最佳K值的交集出发我们确定K为6，在新的K值下

**普通KNN算法**有如下输出，较原有提高了3个百分点达到了66%的正确率

```
===============================

全部样本准确率为66.66666666666666%

类别0的精确率为68.75%
类别1的精确率为80.0%

类别2的精确率为50.0%

类别0的召回率为100.0%
类别1的召回率为57.14285714285714%
类别2的召回率为45.45454545454545%
[[11, 0, 0], [1, 8, 5], [4, 2, 5]]
```

|            | 识别为第一类样本 | 识别为第二类样本 | 识别为第三类样本 |
| ---------- | ---------------- | ---------------- | ---------------- |
| 第一类样本 | 11               | 0                | 0                |
| 第二类样本 | 1                | 8                | 5                |
| 第三类样本 | 4                | 2                | 5                |

**线性变换后的KNN算法**有如下输出

Loss损失函数

![image-20220504162747120](https://markdown-image-1302476306.cos.ap-nanjing.myqcloud.com/202205041627269.png)

```
===============================
全部样本准确率为69.44444444444444%
===============================
类别0的精确率为68.75%
类别1的精确率为88.88888888888889%
类别2的精确率为54.54545454545454%
===============================
类别0的召回率为100.0%
类别1的召回率为57.14285714285714%
类别2的召回率为54.54545454545454%
[[11, 0, 0], [1, 8, 5], [4, 1, 6]]
```

|            | 识别为第一类样本 | 识别为第二类样本 | 识别为第三类样本 |
| ---------- | ---------------- | ---------------- | ---------------- |
| 第一类样本 | 11               | 0                | 0                |
| 第二类样本 | 1                | 8                | 5                |
| 第三类样本 | 4                | 1                | 6                |

在新的K值下线性变换带来的效果提升明显下降

### 度量方式的选择

```
===============================
全部样本准确率为72.22%
===============================
类别0的精确率为71.43%
类别1的精确率为100.00%
类别2的精确率为57.14%
===============================
类别0的召回率为90.91%
类别1的召回率为57.14%
类别2的召回率为72.73%
[[10, 0, 1], [1, 8, 5], [3, 0, 8]]
```

曼哈顿距离在当前的K值下，使得准确率达到了72%

```
===============================
全部样本准确率为36.11%
===============================
无预测为0类别的结果
类别1的精确率为36.36%
类别2的精确率为35.71%
===============================
类别0的召回率为0.00%
类别1的召回率为57.14%
类别2的召回率为45.45%
[[0, 8, 3], [0, 8, 6], [0, 6, 5]]
```

汉明顿距离下在当前K值下，准确率下降到36%

```
===============================
全部样本准确率为66.67%
===============================
类别0的精确率为66.67%
类别1的精确率为80.00%
类别2的精确率为54.55%
===============================
类别0的召回率为90.91%
类别1的召回率为57.14%
类别2的召回率为54.55%
[[10, 1, 0], [1, 8, 5], [4, 1, 6]]
```

在用L无穷范书作为距离下，当前K值准确率为66%

因而欧拉距离、曼哈顿距离或者Loo较为合适



## 参考资料

[KNN与交叉验证](https://www.jianshu.com/p/21483858fcf6)
